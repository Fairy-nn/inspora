package feed

import (
	"context"
	"encoding/json"
	"log"
	"time"

	"github.com/Fairy-nn/inspora/internal/domain"
	"github.com/Fairy-nn/inspora/internal/repository"
	"github.com/IBM/sarama"
)

const (
	// ConsumerGroupID æ˜¯ Feed æ¶ˆè´¹è€…ç»„çš„ ID
	ConsumerGroupID = "feed_consumer_group"
	// MaxFanoutBatchSize æ˜¯æ¯æ‰¹å¤„ç†çš„æœ€å¤§ç²‰ä¸æ•°é‡
	MaxFanoutBatchSize = 1000
	// MaxFollowerForPush æ˜¯ä½¿ç”¨æ¨æ¨¡å‹çš„æœ€å¤§ç²‰ä¸æ•°é‡é˜ˆå€¼
	// å¦‚æœç²‰ä¸æ•°é‡è¶…è¿‡æ­¤å€¼ï¼Œå°†ä¸ä¼šç›´æ¥æ¨é€åˆ°æ‰€æœ‰ç²‰ä¸çš„æ”¶ä»¶ç®±
	MaxFollowerForPush = 100000
)

// FeedConsumer æ˜¯ Feed äº‹ä»¶æ¶ˆè´¹è€…æ¥å£
type Consumer interface {
	// Start å¯åŠ¨æ¶ˆè´¹è€…
	Start(ctx context.Context) error
}

// KafkaFeedConsumer æ˜¯åŸºäº Kafka çš„ Feed äº‹ä»¶æ¶ˆè´¹è€…
type KafkaFeedConsumer struct {
	client      sarama.ConsumerGroup
	feedRepo    repository.FeedRepository
	followRepo  repository.FollowRepository
	articleRepo repository.ArticleRepository
	userRepo    repository.UserRepositoryInterface
}

// NewKafkaFeedConsumer åˆ›å»ºä¸€ä¸ªæ–°çš„ KafkaFeedConsumer
func NewKafkaFeedConsumer(
	client sarama.Client,
	feedRepo repository.FeedRepository,
	followRepo repository.FollowRepository,
	articleRepo repository.ArticleRepository,
	userRepo repository.UserRepositoryInterface,
) Consumer {
	consumerGroup, err := sarama.NewConsumerGroupFromClient(ConsumerGroupID, client)
	if err != nil {
		panic(err)
	}

	return &KafkaFeedConsumer{
		client:      consumerGroup,
		feedRepo:    feedRepo,
		followRepo:  followRepo,
		articleRepo: articleRepo,
		userRepo:    userRepo,
	}
}

// Start å¯åŠ¨æ¶ˆè´¹è€…
func (k *KafkaFeedConsumer) Start(ctx context.Context) error {
	log.Printf("Feed æ¶ˆè´¹è€…å¼€å§‹å¯åŠ¨ï¼Œè®¢é˜…ä¸»é¢˜: %s", FeedTopic)
	topics := []string{FeedTopic}
	for {
		select {
		case <-ctx.Done():
			log.Printf("Feed æ¶ˆè´¹è€…æ”¶åˆ°åœæ­¢ä¿¡å·ï¼Œå³å°†é€€å‡º")
			return ctx.Err()
		default:
			log.Printf("Feed æ¶ˆè´¹è€…å¼€å§‹ä¸€è½®æ¶ˆè´¹")
			err := k.client.Consume(ctx, topics, k)
			if err != nil {
				log.Printf("Feed æ¶ˆè´¹é”™è¯¯: %vï¼Œå°†åœ¨5ç§’åé‡è¯•", err)
				time.Sleep(time.Second * 5) // å‡ºé”™åç­‰å¾…ä¸€æ®µæ—¶é—´å†é‡è¯•
			}
		}
	}
}

// Setup æ˜¯åœ¨æ¶ˆè´¹è€…ä¼šè¯å¼€å§‹å‰è°ƒç”¨çš„é’©å­
func (k *KafkaFeedConsumer) Setup(sarama.ConsumerGroupSession) error {
	log.Printf("Feed æ¶ˆè´¹è€…ä¼šè¯å·²åˆå§‹åŒ–")
	return nil
}

// Cleanup æ˜¯åœ¨æ¶ˆè´¹è€…ä¼šè¯ç»“æŸåè°ƒç”¨çš„é’©å­
func (k *KafkaFeedConsumer) Cleanup(sarama.ConsumerGroupSession) error {
	log.Printf("Feed æ¶ˆè´¹è€…ä¼šè¯å·²æ¸…ç†")
	return nil
}

// ConsumeClaim å¤„ç†ä» Kafka æ¥æ”¶åˆ°çš„æ¶ˆæ¯
func (k *KafkaFeedConsumer) ConsumeClaim(session sarama.ConsumerGroupSession, claim sarama.ConsumerGroupClaim) error {
	log.Printf("Feed æ¶ˆè´¹è€…å¼€å§‹å¤„ç†åˆ†åŒº %d çš„æ¶ˆæ¯", claim.Partition())
	for msg := range claim.Messages() {
		log.Printf("æ”¶åˆ°æ–°çš„ Feed äº‹ä»¶æ¶ˆæ¯: åç§»=%d, åˆ†åŒº=%d", msg.Offset, msg.Partition)

		if err := k.processFeedEvent(session.Context(), msg); err != nil {
			log.Printf("å¤„ç† Feed äº‹ä»¶å¤±è´¥: %v", err)
		} else {
			log.Printf("æˆåŠŸå¤„ç† Feed äº‹ä»¶æ¶ˆæ¯: åç§»=%d, åˆ†åŒº=%d", msg.Offset, msg.Partition)
		}

		session.MarkMessage(msg, "")
	}
	return nil
}

// processFeedEvent å¤„ç†å•ä¸ª Feed äº‹ä»¶
func (k *KafkaFeedConsumer) processFeedEvent(ctx context.Context, msg *sarama.ConsumerMessage) error {
	// è§£æäº‹ä»¶
	var event domain.FeedEvent
	if err := json.Unmarshal(msg.Value, &event); err != nil {
		log.Printf("è§£æ Feed äº‹ä»¶å¤±è´¥: %v", err)
		return err
	}

	log.Printf("ğŸ” å¤„ç† Feed äº‹ä»¶: ç±»å‹=%s, ID=%d, ç”¨æˆ·ID=%d", event.EventType, event.ID, event.UserID)

	switch event.EventType {
	case EventTypeArticlePublished:
		log.Printf("å¼€å§‹å¤„ç†æ–‡ç« å‘å¸ƒäº‹ä»¶: ID=%d", event.ID)
		err := k.handleArticlePublished(ctx, event)
		if err != nil {
			log.Printf(" æ–‡ç« å‘å¸ƒäº‹ä»¶å¤„ç†å¤±è´¥: %v", err)
		} else {
			log.Printf("æ–‡ç« å‘å¸ƒäº‹ä»¶å¤„ç†æˆåŠŸ: ID=%d", event.ID)
		}
		return err

	case EventTypeArticleLiked:
		log.Printf("å¼€å§‹å¤„ç†æ–‡ç« ç‚¹èµäº‹ä»¶: ID=%d", event.ID)
		err := k.handleArticleLiked(ctx, event)
		if err != nil {
			log.Printf("æ–‡ç« ç‚¹èµäº‹ä»¶å¤„ç†å¤±è´¥: %v", err)
		} else {
			log.Printf("æ–‡ç« ç‚¹èµäº‹ä»¶å¤„ç†æˆåŠŸ: ID=%d", event.ID)
		}
		return err

	case EventTypeUserFollowed:
		log.Printf("å¼€å§‹å¤„ç†ç”¨æˆ·å…³æ³¨äº‹ä»¶: ID=%d", event.ID)
		err := k.handleUserFollowed(ctx, event)
		if err != nil {
			log.Printf("ç”¨æˆ·å…³æ³¨äº‹ä»¶å¤„ç†å¤±è´¥: %v", err)
		} else {
			log.Printf("ç”¨æˆ·å…³æ³¨äº‹ä»¶å¤„ç†æˆåŠŸ: ID=%d", event.ID)
		}
		return err

	case EventTypeArticleCommented:
		log.Printf(" å¼€å§‹å¤„ç†æ–‡ç« è¯„è®ºäº‹ä»¶: ID=%d", event.ID)
		err := k.handleArticleCommented(ctx, event)
		if err != nil {
			log.Printf("æ–‡ç« è¯„è®ºäº‹ä»¶å¤„ç†å¤±è´¥: %v", err)
		} else {
			log.Printf("æ–‡ç« è¯„è®ºäº‹ä»¶å¤„ç†æˆåŠŸ: ID=%d", event.ID)
		}
		return err

	case EventTypeArticleCollected:
		log.Printf("å¼€å§‹å¤„ç†æ–‡ç« æ”¶è—äº‹ä»¶: ID=%d", event.ID)
		err := k.handleArticleCollected(ctx, event)
		if err != nil {
			log.Printf(" æ–‡ç« æ”¶è—äº‹ä»¶å¤„ç†å¤±è´¥: %v", err)
		} else {
			log.Printf("æ–‡ç« æ”¶è—äº‹ä»¶å¤„ç†æˆåŠŸ: ID=%d", event.ID)
		}
		return err

	default:
		// æœªçŸ¥äº‹ä»¶ç±»å‹ï¼Œå¿½ç•¥
		log.Printf("æ”¶åˆ°æœªçŸ¥äº‹ä»¶ç±»å‹: %s, å·²å¿½ç•¥", event.EventType)
		return nil
	}
}

// handleArticlePublished å¤„ç†æ–‡ç« å‘å¸ƒäº‹ä»¶
func (k *KafkaFeedConsumer) handleArticlePublished(ctx context.Context, event domain.FeedEvent) error {
	// ä»äº‹ä»¶ä¸­è·å–ä¿¡æ¯
	authorID := event.UserID
	articleID, ok := event.Content["article_id"].(float64)
	if !ok {
		log.Printf("æ–‡ç« å‘å¸ƒäº‹ä»¶æ ¼å¼ä¸æ­£ç¡®: article_id ä¸å­˜åœ¨æˆ–æ ¼å¼é”™è¯¯")
		return nil // è·³è¿‡æ ¼å¼ä¸æ­£ç¡®çš„äº‹ä»¶
	}

	log.Printf("å¤„ç†æ–‡ç« å‘å¸ƒäº‹ä»¶: ä½œè€…ID=%d, æ–‡ç« ID=%d", authorID, int64(articleID))

	// åˆ›å»º FeedItem
	item, err := domain.NewUserFeedItem(
		"article",
		int64(articleID),
		authorID,
		event.Ctime,
		map[string]interface{}{
			"type":       "article_published",
			"article_id": articleID,
			"author_id":  authorID,
		},
	)
	if err != nil {
		log.Printf("åˆ›å»º Feed é¡¹å¤±è´¥: %v", err)
		return err
	}

	if err := k.feedRepo.AddToOutbox(ctx, authorID, item); err != nil {
		log.Printf("æ·»åŠ åˆ°å‘ä»¶ç®±å¤±è´¥: %v", err)
		return err
	}
	log.Printf("å·²æ·»åŠ åˆ°ä½œè€…(ID=%d)çš„å‘ä»¶ç®±", authorID)

	// 2. è·å–ä½œè€…çš„ç²‰ä¸æ•°é‡
	followerStats, err := k.followRepo.GetStatistics(ctx, authorID)
	if err != nil {
		log.Printf("è·å–ç²‰ä¸ç»Ÿè®¡å¤±è´¥: %v", err)
		return err
	}
	log.Printf("ä½œè€…(ID=%d)çš„ç²‰ä¸æ•°é‡: %d", authorID, followerStats.Followers)

	// å¦‚æœç²‰ä¸æ•°é‡è¶…è¿‡é˜ˆå€¼ï¼Œåˆ™ä¸ä½¿ç”¨æ¨æ¨¡å‹ï¼Œä¾é æ‹‰æ¨¡å‹
	if followerStats.Followers > MaxFollowerForPush {
		// ä¸ºå¤§Vç”¨æˆ·åªæ¨é€ç»™ä¸€éƒ¨åˆ†æ´»è·ƒç²‰ä¸ï¼Œæˆ–è€…å®Œå…¨ä¸æ¨é€
		log.Printf("ç”¨æˆ· %d æ˜¯å¤§V (ç²‰ä¸: %d)ï¼Œä¸è¿›è¡Œå…¨é¢æ¨é€", authorID, followerStats.Followers)
		return nil
	}

	// 3. è·å–ä½œè€…çš„ç²‰ä¸åˆ—è¡¨å¹¶æ¨é€
	offset := int64(0)
	limit := int64(MaxFanoutBatchSize)

	totalFanoutCount := 0
	for {
		followers, err := k.followRepo.GetFollowerList(ctx, authorID, offset, limit)
		if err != nil {
			log.Printf("è·å–ç²‰ä¸åˆ—è¡¨å¤±è´¥: %v", err)
			return err
		}

		if len(followers) == 0 {
			log.Printf("æ²¡æœ‰æ›´å¤šç²‰ä¸ï¼Œæ¨é€ç»“æŸ")
			break // æ²¡æœ‰æ›´å¤šç²‰ä¸äº†
		}

		log.Printf("å¼€å§‹å‘ %d åç²‰ä¸æ¨é€ Feed", len(followers))

		// æ‰¹é‡æ¨é€åˆ°ç²‰ä¸çš„æ”¶ä»¶ç®±
		successCount := 0
		for _, follower := range followers {
			if err := k.feedRepo.AddToInbox(ctx, follower.Follower, item); err != nil {
				log.Printf("æ¨é€åˆ°ç”¨æˆ· %d çš„æ”¶ä»¶ç®±å¤±è´¥: %v", follower.Follower, err)
				continue
			}
			successCount++
		}

		totalFanoutCount += successCount
		log.Printf("æˆåŠŸæ¨é€ç»™ %d/%d åç²‰ä¸", successCount, len(followers))

		if len(followers) < int(limit) {
			log.Printf("ç²‰ä¸æ‰¹æ¬¡ä¸è¶³ï¼Œæ¨é€ç»“æŸ")
			break // æ²¡æœ‰æ›´å¤šç²‰ä¸äº†
		}

		offset += limit
	}

	log.Printf("æ–‡ç« å‘å¸ƒäº‹ä»¶å¤„ç†å®Œæˆï¼Œå…±æ¨é€ç»™ %d åç²‰ä¸", totalFanoutCount)

	return nil
}

// handleArticleLiked å¤„ç†æ–‡ç« ç‚¹èµäº‹ä»¶
func (k *KafkaFeedConsumer) handleArticleLiked(ctx context.Context, event domain.FeedEvent) error {
	// ä»äº‹ä»¶ä¸­è·å–ä¿¡æ¯
	likerID := event.UserID
	articleID, _ := event.Content["article_id"].(float64)
	authorID, _ := event.Content["author_id"].(float64)

	// åˆ›å»º FeedItem
	item, err := domain.NewUserFeedItem(
		"like",
		int64(articleID),
		likerID,
		event.Ctime,
		map[string]interface{}{
			"type":       "article_liked",
			"article_id": articleID,
			"liker_id":   likerID,
			"author_id":  authorID,
		},
	)
	if err != nil {
		return err
	}

	// å°†äº‹ä»¶æ·»åŠ åˆ°è¢«ç‚¹èµæ–‡ç« ä½œè€…çš„æ”¶ä»¶ç®± (ä»…é€šçŸ¥ä½œè€…)
	return k.feedRepo.AddToInbox(ctx, int64(authorID), item)
}

// handleUserFollowed å¤„ç†ç”¨æˆ·å…³æ³¨äº‹ä»¶
func (k *KafkaFeedConsumer) handleUserFollowed(ctx context.Context, event domain.FeedEvent) error {
	// ä»äº‹ä»¶ä¸­è·å–ä¿¡æ¯
	followerID := event.UserID
	followeeID, _ := event.Content["followee_id"].(float64)

	// åˆ›å»º FeedItem
	item, err := domain.NewUserFeedItem(
		"follow",
		int64(followeeID), // ä½¿ç”¨è¢«å…³æ³¨è€…IDä½œä¸ºé¡¹ç›®ID
		followerID,
		event.Ctime,
		map[string]interface{}{
			"type":        "user_followed",
			"follower_id": followerID,
			"followee_id": followeeID,
		},
	)
	if err != nil {
		return err
	}

	if err := k.feedRepo.AddToInbox(ctx, int64(followeeID), item); err != nil {
		return err
	}

	return nil
}

// handleArticleCommented å¤„ç†æ–‡ç« è¯„è®ºäº‹ä»¶
func (k *KafkaFeedConsumer) handleArticleCommented(ctx context.Context, event domain.FeedEvent) error {
	// ä»äº‹ä»¶ä¸­è·å–ä¿¡æ¯
	commenterID := event.UserID
	articleID, _ := event.Content["article_id"].(float64)
	commentID, _ := event.Content["comment_id"].(float64)
	commentContent, _ := event.Content["comment_content"].(string)
	authorID, _ := event.Content["author_id"].(float64)

	// æˆªå–è¯„è®ºå†…å®¹ (å¦‚æœå¤ªé•¿)
	if len(commentContent) > 50 {
		commentContent = commentContent[:50] + "..."
	}

	// åˆ›å»º FeedItem
	item, err := domain.NewUserFeedItem(
		"comment",
		int64(commentID),
		commenterID,
		event.Ctime,
		map[string]interface{}{
			"type":            "article_commented",
			"article_id":      articleID,
			"commenter_id":    commenterID,
			"author_id":       authorID,
			"comment_id":      commentID,
			"comment_content": commentContent,
		},
	)
	if err != nil {
		return err
	}

	// å°†è¯„è®ºäº‹ä»¶æ·»åŠ åˆ°è¢«è¯„è®ºæ–‡ç« ä½œè€…çš„æ”¶ä»¶ç®± (ä»…é€šçŸ¥ä½œè€…)
	// å¦‚æœè¯„è®ºè€…å°±æ˜¯ä½œè€…è‡ªå·±ï¼Œåˆ™ä¸éœ€è¦é¢å¤–é€šçŸ¥
	if commenterID != int64(authorID) {
		return k.feedRepo.AddToInbox(ctx, int64(authorID), item)
	}

	return nil
}

// handleArticleCollected å¤„ç†æ–‡ç« æ”¶è—äº‹ä»¶
func (k *KafkaFeedConsumer) handleArticleCollected(ctx context.Context, event domain.FeedEvent) error {
	// ä»äº‹ä»¶ä¸­è·å–ä¿¡æ¯
	collectorID := event.UserID
	articleID, _ := event.Content["article_id"].(float64)
	authorID, _ := event.Content["author_id"].(float64)

	// åˆ›å»º FeedItem
	item, err := domain.NewUserFeedItem(
		"collect",
		int64(articleID),
		collectorID,
		event.Ctime,
		map[string]interface{}{
			"type":         "article_collected",
			"article_id":   articleID,
			"collector_id": collectorID,
			"author_id":    authorID,
		},
	)
	if err != nil {
		return err
	}

	// å°†æ”¶è—äº‹ä»¶æ·»åŠ åˆ°æ–‡ç« ä½œè€…çš„æ”¶ä»¶ç®± (ä»…é€šçŸ¥ä½œè€…)
	// å¦‚æœæ”¶è—è€…å°±æ˜¯ä½œè€…è‡ªå·±ï¼Œåˆ™ä¸éœ€è¦é¢å¤–é€šçŸ¥
	if collectorID != int64(authorID) {
		return k.feedRepo.AddToInbox(ctx, int64(authorID), item)
	}

	return nil
}
